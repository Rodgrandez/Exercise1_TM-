{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', 80)\n",
    "\n",
    "from re import sub, split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE, MDS\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "def plot_embedding(V, y):\n",
    "    \"\"\" Visualizes a vocabulary embedding via TSNE \"\"\"\n",
    "    V = TruncatedSVD(50).fit_transform(V)\n",
    "    d = TSNE(metric='cosine').fit_transform(V)\n",
    "    d = pd.DataFrame(d).assign(label = y.reset_index(drop=True))\n",
    "    return sns.scatterplot(x = 0, y = 1, hue = 'label', data = d), d\n",
    "\n",
    "\n",
    "def clean_twitter(s):\n",
    "    \"\"\" Cleans Twitter specific issues\n",
    "    Should probably clean out mentions, URLs, and RT's.\n",
    "    \"\"\"\n",
    "# Use regular expressions to remove unwanted\n",
    "# Remove : @,https, RTs, all non letter characters (#,numbers,emojis,etc),\n",
    "# then, remove single character and multiple spaces.\n",
    "    pat1=r'@\\S+|https?[^\\s]+|RT\\s+|[^\\w+]'\n",
    "    pat2=r'\\s+[a-zA-Z]\\s+|\\s+'\n",
    "    pat3=r'^\\s+|\\s+$'\n",
    "    tex_clean=sub(pat1,\" \", s)\n",
    "    text_space=sub(pat2,\" \", tex_clean)\n",
    "    output = sub(pat3,\"\", text_space)\n",
    "# library \"spacy\" to\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    sentence=sp(output)\n",
    "    # Extract part-of-speech and lemmatize this sentence.\n",
    "    part_of_speech = [(x.orth_,x.pos_, x.lemma_) for x in [y \n",
    "                                      for y\n",
    "                                      in sentence \n",
    "                                      if not y.is_stop and y.pos_ != 'PUNCT']]\n",
    "    #Named entity extraction \n",
    "    entity = dict([(str(x), x.label_) for x in sentence.ents])\n",
    "    #lemmatize\n",
    "    lemmatize=\" \".join([word.lemma_ for word in sentence])\n",
    "    # I Return the sentence lemmatized. \n",
    "    return lemmatize\n",
    "        \n",
    "\n",
    "    # TODO: Use regular expressions to remove unwanted\n",
    "    # text and clean up our tweets to be more usable!\n",
    "\n",
    "    # BONUS: Try using the library \"spacy\" to \n",
    "    # do further processing, such as lemmatizing\n",
    "    # or replacing Named Entities with constants (i.e. \"[NAMED]\")\n",
    "    # or adding the part of speech or dependency code to the word \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/tweets.csv').tweet\n",
    "y = pd.read_csv('data/tweets.csv').label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    here CNN on Sharia law -PRON- can be stone or have -PRON- hand cut off but b...\n",
       "1    LOOK obama Clinton crony ILLEGALLY arm amp train muslim terrorist include is...\n",
       "2    ThrowbackThursday BenGarrison cartoon from 2013 Obama get crown War Debt tax...\n",
       "3                                    Say Islam be peace or else trump maga isis tcot\n",
       "4                                                all aboard the Trump Train ChooChoo\n",
       "5           FLASHBACK gt gt Judicial Watch Releases Huma Abedin Deposition testimony\n",
       "6                follow FBI presser say the system be rig amp weigh in foxldt 7 p.m.\n",
       "7                                           trump -PRON- re run Against Rigged Press\n",
       "8    literally 98 of Hillary supporter see online be astroturfe spammer with 12 0...\n",
       "9                                                                              Islam\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:10].map(clean_twitter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our data by using nothing but the Sklearn default\n",
    "# cleaning and tokenizing\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "V = vectorizer.fit_transform(X)\n",
    "ax, d = plot_embedding(V, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see what our cleaning has done\n",
    "vectorizer = CountVectorizer(preprocessor = clean_twitter)\n",
    "V = vectorizer.fit_transform(X)\n",
    "ax, d = plot_embedding(V, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "french_stop_words = set(stopwords.words('french'))\n",
    "stw=stop_words.union(french_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try with TF-IDF vectorizer, and add implicit stopwords!\n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor = clean_twitter,\n",
    "                                   stop_words=stw)\n",
    "V = tfidf_vectorizer.fit_transform(X)\n",
    "ax, d = plot_embedding(V, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Can you get things to separate in the space in a better way?\n",
    "#As usual the best way to adjust the feature extraction parameters is to use a \n",
    "#cross-validated grid search. \n",
    "# In this case for example y use some values of the hyperparameters and get to separate in the space a lit better. \n",
    "tfidf_vectorizer = TfidfVectorizer(preprocessor = clean_twitter,\n",
    "                                   max_df=0.75,\n",
    "                                   max_features=10000,\n",
    "                                   stop_words=stw)\n",
    "V = tfidf_vectorizer.fit_transform(X)\n",
    "ax, d = plot_embedding(V, y)\n",
    "# Can you get things to separate in the space in a better way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try with Hashing vectorizer, and add implicit stopwords!\n",
    "Hash_vectorizer = HashingVectorizer(preprocessor = clean_twitter,\n",
    "                                     stop_words=stw)\n",
    "V = Hash_vectorizer.fit_transform(X)\n",
    "ax, d = plot_embedding(V, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
